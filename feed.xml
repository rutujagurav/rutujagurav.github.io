<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rutujagurav.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rutujagurav.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-10T20:27:18+00:00</updated><id>https://rutujagurav.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">what’s going on with sktime?</title><link href="https://rutujagurav.github.io/blog/2024/aeon-sktime-drama/" rel="alternate" type="text/html" title="what’s going on with sktime?"/><published>2024-04-18T23:00:00+00:00</published><updated>2024-04-18T23:00:00+00:00</updated><id>https://rutujagurav.github.io/blog/2024/aeon-sktime-drama</id><content type="html" xml:base="https://rutujagurav.github.io/blog/2024/aeon-sktime-drama/"><![CDATA[<p>It is super late at night and I have fallen into an internet rabbit hole of the timeseries machine learning community. I’ve been working with a lot of time domain data for a while now and of the several good python libraries available, <a href="https://www.sktime.net/en/stable/"><code class="language-plaintext highlighter-rouge">sktime</code></a> has been one that I haven’t quite used too much. My go-to ones have been - 1. <a href="https://tsfresh.readthedocs.io/en/latest/"><code class="language-plaintext highlighter-rouge">tsfresh</code></a> for the feature extraction capabilities, 2. <a href="https://tslearn.readthedocs.io/en/stable/"><code class="language-plaintext highlighter-rouge">tslearn</code></a> for standard machine learning algorithms, 3. <a href="https://stumpy.readthedocs.io/en/latest/"><code class="language-plaintext highlighter-rouge">stumpy</code></a> for all things matrix profile and 4. <a href="https://timeseriesai.github.io/tsai/"><code class="language-plaintext highlighter-rouge">tsai</code></a> for deep learning based algorithms.</p> <p>I have been playing around with a short project idea of examining clustering perfromance on all available timeseries datasets in the mighty UCR/UEA timeseries classification archive using various timeseries feature-sets like the ones available in <code class="language-plaintext highlighter-rouge">tsfresh</code> and the increasingly popular Catch-22 features available in <code class="language-plaintext highlighter-rouge">pycatch22</code> package<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, so imagine my delight when I went to <a href="https://timeseriesclassification.com">timeseriesclassification.com</a> to fetch all the datasets and read <em>“The scikit-learn compatible aeon toolkit contains the state of the art algorithms for time series classification. All of the datasets and results stored here are directly accessible in code using aeon.”</em> I thought, “Awesome, eveything I need in one place!”. So I go check out <a href="https://www.aeon-toolkit.org/en/stable/"><code class="language-plaintext highlighter-rouge">aeon</code></a> and it is fantastic. But my brain must have done a random access of some forgotten recess of my mind because I found myself thinking, “Huh, this looks familiar. Almost like <code class="language-plaintext highlighter-rouge">sktime</code>…”. And indeed that’s when I fell into the current rabbit hole from whence I write this post.</p> <p>Turns out there is some drama-llama stuff around this whole <code class="language-plaintext highlighter-rouge">sktime</code> vs. <code class="language-plaintext highlighter-rouge">aeon</code> saga. Turns out <a href="https://news.ycombinator.com/item?id=36432369"><code class="language-plaintext highlighter-rouge">aeon</code> is a fork of sktime</a> created by Tony Bagnall of UEA who departed(?) from <code class="language-plaintext highlighter-rouge">sktime</code> after <a href="https://github.com/sktime/community-org/issues/45">another core developer</a> allegedly took over the project and kicked others out(?) after fallout over some financial issues(?). The whole thing sounds like a bit of a mess. Anyhoo, I have no horse in the race. Preliminary examination suggests either library is fine for my purposes. I am going to go with <code class="language-plaintext highlighter-rouge">aeon</code> for the aforementioned short project. And this Alice needs to climb out of this hole and go to bed now.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I know, I know, some of the datasets available in the UCR/UEA archive are not amenable to features based classification perhaps and are more separable in terms of shapes or a combination of both but I digress. My plan is to see consensus between clustering and classification performance on the datasets. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="oh-what-a-mess"/><category term="uff"/><category term="musings"/><category term="sktime"/><category term="aeon"/><summary type="html"><![CDATA[the saga of sktime and aeon]]></summary></entry><entry><title type="html">convenience wrappers</title><link href="https://rutujagurav.github.io/blog/2024/wrappers-for-clf-and-clust/" rel="alternate" type="text/html" title="convenience wrappers"/><published>2024-04-15T15:00:00+00:00</published><updated>2024-04-15T15:00:00+00:00</updated><id>https://rutujagurav.github.io/blog/2024/wrappers-for-clf-and-clust</id><content type="html" xml:base="https://rutujagurav.github.io/blog/2024/wrappers-for-clf-and-clust/"><![CDATA[<h1 id="getting-started">Getting Started</h1> <p>Every time I start a new machine learning project (not deep learning, that’s a story for another time), I find myself going through the same tedious process of trial and error - set up a grid search to find the <em>right</em> model along with the <em>right</em> set of hyperparameters for the model that optimize one or more of the laundry list of <em>metrics-of-interest</em>…then repeat every combination of <em>free parameters</em> in this pipeline a bunch of times and finally making a lot of plots to get the lay of the land so to speak. So, over the years, I’ve developed a set of convenience wrappers around the mighty <code class="language-plaintext highlighter-rouge">scikit-learn</code> library to make this process a bit more streamlined and I’ve published them as <code class="language-plaintext highlighter-rouge">clfutils4r</code> for classification tasks and <code class="language-plaintext highlighter-rouge">clustutils4r</code> for clustering tasks (the ‘r’ being my initial and not the programming language…err, should have thought this through, huh?). I thought I would share them here in case they are useful to anyone else.</p> <h2 id="classification">Classification</h2> <p>The premise is this: Someone hands you a classification dataset. After you are done poking and prodding it with some exploratory data analysis (EDA), you want to know the standard metrics on various classifiers available in <code class="language-plaintext highlighter-rouge">scikit-learn</code> and you want to know them <em>now</em>. You don’t want to spend time writing boilerplate code setting up a grid search and you don’t want to spend time making plots to consolidate the results of said grid search. Here is minimally complete example of how you can do just that with 2 wrapper functions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1">## Load dataset: Example - breast cancer prediction
</span><span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_breast_cancer</span><span class="p">()</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">target_names</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">]</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">target</span>

<span class="c1">## Standardize features
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">## Split into train and test sets
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">## Grid search for best model
</span><span class="kn">from</span> <span class="n">clfutils4r.gridsearch_classification</span> <span class="kn">import</span> <span class="n">gridsearch_classification</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">gridsearch_results</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">best_model</span><span class="p">,</span> <span class="n">grid_search_results</span> <span class="o">=</span> <span class="nf">gridsearch_classification</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>                    <span class="c1"># training dataset
</span>                                                            <span class="n">gt_labels</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>            <span class="c1"># ground truth labels
</span>                                                            <span class="n">best_model_metric</span><span class="o">=</span><span class="sh">"</span><span class="s">F1</span><span class="sh">"</span><span class="p">,</span>       <span class="c1"># metric to use to choose the best model
</span>                                                            <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                    <span class="c1"># whether to display the plots; this is used in a notebook
</span>                                                            <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="n">save_dir</span>  <span class="c1"># whether to save the plots
</span>                                                        <span class="p">)</span>

<span class="c1">## Predict on test set
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">## Evaluate best model on test set
</span><span class="kn">from</span> <span class="n">clfutils4r.eval_classification</span> <span class="kn">import</span> <span class="n">eval_classification</span>
<span class="nf">eval_classification</span><span class="p">(</span><span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="o">=</span><span class="n">y_pred_proba</span><span class="p">,</span>  <span class="c1"># ground truth labels, predicted labels, predicted probabilities
</span>                    <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
                    <span class="n">make_metrics_plots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># make a variety of classification metrics plots
</span>                    <span class="n">make_shap_plot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shap_nsamples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="c1"># do Shapley analysis for model explainability
</span>                    <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  
                    <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">RESULTS_DIR</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getcwd</span><span class="p">()</span><span class="o">+</span><span class="sh">'</span><span class="s">/test_results</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Let’s dive a bit deeper into these two convenience functions.</p> <h3 id="gridsearch_classification"><code class="language-plaintext highlighter-rouge">gridsearch_classification</code></h3> <p>This will produce a whole bunch of useful outputs including the best model and the results of the grid search. The data is stored in neat folder structure in JSON files and is visualized with a <em>Parallel Co-ordinates Plot</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The parallel co-ordinates plot that is produced by gridsearch_classification() for the k-Nearest Neighbors classifier for the example above. </div> <h3 id="eval_classification"><code class="language-plaintext highlighter-rouge">eval_classification</code></h3> <p>The primary output is the classic sklearn <em>classification report</em>. Sometimes that’s all you need but by setting the <code class="language-plaintext highlighter-rouge">make_metrics_plots</code> to <code class="language-plaintext highlighter-rouge">True</code> you can choose to make a variety of other plots that I find useful for understanding the performance of the model. These include the familiar plots of the confusion matrix, the ROC curve, the precision-recall curve as well as some more exotic ones I found in <code class="language-plaintext highlighter-rouge">scikit-plot</code> that are exclusive to binary classification like the KS statistic plot, the cumulative gains curve and the lift curve. You can also choose to do Shapley analysis to <em>explain</em> the model predictions by setting the <code class="language-plaintext highlighter-rouge">make_shap_plot</code> parameter to <code class="language-plaintext highlighter-rouge">True</code> and specifying the number of samples to use for the analysis with the <code class="language-plaintext highlighter-rouge">shap_nsamples</code> parameter. I love the fantastic <code class="language-plaintext highlighter-rouge">shap</code> package so I just wrapped the <em>KernelExplainer</em> and <em>summary_plot</em> in this function.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The classic evaluation plots produced by eval_classification() on the test set for the best model returned by gridsearch_classification() for the example above. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Some more exotic evaluation plots exclusive to binary classification produced by eval_classification() on the test set for the best model returned by gridsearch_classification() for the example above. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Shapley analysis summary plot produced by shap package for the best model returned by gridsearch_classification() for the example above. </div> <h2 id="clustering">Clustering</h2> <p>Same premise as before but this time with clustering tasks. Here is a minimally complete example of how you can do it with 1 function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="n">clustutils4r.eval_clustering</span> <span class="kn">import</span> <span class="n">eval_clustering</span>

<span class="c1">## For testing purposes
</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span>

<span class="c1">### Synthetic data
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">save_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">results</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">best_model</span><span class="p">,</span> <span class="n">grid_search_results</span> <span class="o">=</span> <span class="nf">eval_clustering</span><span class="p">(</span>
                                       <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>                                               <span class="c1"># dataset to cluster
</span>                                       <span class="n">gt_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>                                       <span class="c1"># ground-truth labels; often these aren't available so don't pass this argument
</span>                                       <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>                                       <span class="c1"># number of times to fit a model
</span>                                       <span class="n">best_model_metric</span><span class="o">=</span><span class="sh">"</span><span class="s">Calinski-Harabasz</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># metric to use to choose the best model
</span>                                       <span class="n">make_silhoutte_plots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">embed_data_in_2d</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># whether to make silhouette plots
</span>                                       <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                                         <span class="c1"># whether to display the plots; this is used in a notebook
</span>                                       <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">results</span><span class="sh">"</span>                      <span class="c1"># whether to save the plots
</span>                                    <span class="p">)</span>
</code></pre></div></div> <p>Clustering, as you know, is a bit trickier than classification because often there is no ground truth to compare the found clusters to. There is also rarely some external validation test or downstream task available to quantify if any/all clusters you found are “useful”. Clustering is often an exploratory tool. So, the evaluation is a bit more heuristic based. Clustering also often has one or more <em>free parameters</em>, for example, the number of clusters in case of partition-based algorithms like K-Means or the minimum cluster size in case of density based algorithms. In <code class="language-plaintext highlighter-rouge">sklearn</code> there are 3 intrinsic cluster quality metrics viz. the <em>Calinski-Harabasz</em> score, the <em>Davies-Bouldin</em> score and the most used one being the <em>Silhouette Score</em>. Another important evaluation folks do for clustering is measuring the consensus between different labellings of the same dataset. <code class="language-plaintext highlighter-rouge">sklearn</code> has a wide variety of clustering consensus metrics like <em>Adjusted Rand Index</em>, <em>Normalized Mutual Information</em>, <em>V Measure</em>, <em>Fowlkes-Mallows Index</em>.</p> <p>Let’s take a look at my convenience function.</p> <h3 id="eval_clustering"><code class="language-plaintext highlighter-rouge">eval_clustering</code></h3> <p>In the default setting in which all you have is the unlabelled dataset, it will calculate the three intrinsic cluster quality metrics for a variety of models and combinations of free parameters and return the best model based on the scoring metric you choose using the <code class="language-plaintext highlighter-rouge">best_model_metric</code> parameter along with the full grid search results. You can also make a <em>Silhouette Plot</em> for the best model by setting the <code class="language-plaintext highlighter-rouge">make_silhoutte_plots</code> to <code class="language-plaintext highlighter-rouge">True</code> and since most datasets have more than 2 features, you can get a t-SNE projection of the high dimensional datapoints by setting <code class="language-plaintext highlighter-rouge">embed_data_in_2d</code> to <code class="language-plaintext highlighter-rouge">True</code>. If you have the ground truth labels (or just another set of labels obtained by, say, a different clustering run) available, you can pass them to the <code class="language-plaintext highlighter-rouge">gt_labels</code> parameter and it will calculate the clustering consensus metrics.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The parallel co-ordinates plot that is produced by eval_clustering() for the k-Means clustering from the example above. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Silhouette plot that is produced by eval_clustering() for the best model from the example above. </div> <h2 id="parting-thoughts">Parting Thoughts</h2> <p>Every machine learning engineering, researcher and dabbler I’ve met over the years has their own version of such wrappers. I hope someone finds these ones useful. I’ve packaged and published these on PyPI; install them with <code class="language-plaintext highlighter-rouge">pip install clfutils4r</code> and <code class="language-plaintext highlighter-rouge">pip install clustutils4r</code>. The code is available on my Github, fork away and modify to your liking. Even if you don’t like them as they are, hopefully they save you some time by serving as a starting point. Recently, I’ve taken to using <a href="https://optuna.org/">Optuna</a> for hyperparameter optimization and I’m thinking of incorporating that into these wrappers. It has a lot cleverer ways of optimally searching the hyperparameters space than the good old GridSearchCV and RandomizedSearchCV that I’ve been using here. I would like to point to <a href="https://pycaret.org/">PyCaret</a> which is a fantastic low-code, scikit-learn wrapper library that does a lot of what I’ve done here and so much more; it has a truly eye-watering amount of options one can play with and it integrates pretty much every hyperparameter search package available including Optuna, Ray Tune, Hyperopt, Scikit Optimize (which is probably defunct now?). PyCaret has everything one would need for classification, especially the <code class="language-plaintext highlighter-rouge">compare_models()</code> function which is fantastic for getting a quick <em>models x metrics</em> table comparing all available models via cross-validation and <code class="language-plaintext highlighter-rouge">tune_model()</code> which essentially does what my <code class="language-plaintext highlighter-rouge">gridsearch_classification()</code> does and returns the best model and optionally the tuner object from which you can grab the full grid of results. As of this post, I haven’t seen similar functions in the clustering module of PyCaret but I’m sure they are on the way. I’ve used it a few times and it’s great but I wanted to write my own wrappers to understand the process better. Thanks for reading this post!</p>]]></content><author><name></name></author><category term="my-little-helpers"/><category term="classification"/><category term="scikit-learn"/><category term="wrappers"/><summary type="html"><![CDATA[wrappers for sklearn et al for classification and clustering]]></summary></entry></feed>