<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rutujagurav.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rutujagurav.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-12T00:24:36+00:00</updated><id>https://rutujagurav.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">The Gell-Mann Amnesia Effect</title><link href="https://rutujagurav.github.io/blog/2024/gell-mann-amnesia/" rel="alternate" type="text/html" title="The Gell-Mann Amnesia Effect"/><published>2024-11-11T16:20:00+00:00</published><updated>2024-11-11T16:20:00+00:00</updated><id>https://rutujagurav.github.io/blog/2024/gell-mann-amnesia</id><content type="html" xml:base="https://rutujagurav.github.io/blog/2024/gell-mann-amnesia/"><![CDATA[<p>Murray Gell-Mann won the 1969 Nobel Prize in Physics for his work on the theory of elementary particles; naively put, he invented quarks - one of the handful (embarassingly so) of the smallest (as of yet) building blocks of matter. The <em>Gell-Mann Amnesia Effect</em> was coined and descibed by Michael Crichton (of the Jurassic Park fame) in his 2002 talk at the International Leadership Forum criticizing the prevelance of speculation in journalism, evoking the name of his famous physicist friend because “I once discussed it with Murray Gell-Mann, and by dropping a famous name I imply greater importance to myself, and to the effect, than it would otherwise have.” Refreshingly honest.</p> <p>Chrichton describes the effect in his talk as follows:</p> <p><em>You open the newspaper to an article on some subject you know well. In Murray’s case, physics. In mine, show business. You read the article and see the journalist has absolutely no understanding of either the facts or the issues. Often, the article is so wrong it actually presents the story backward – reversing cause and effect. I call these the “wet streets cause rain” stories. Paper’s full of them. In any case, you read with exasperation or amusement the multiple errors in a story, and then turn the page to national or international affairs, and read as if the rest of the newspaper was somehow more accurate about Palestine than the baloney you just read. You turn the page, and forget what you know.</em></p> <p>I know nothing of the business of journalism except consuming its products in a daily ritual like any other bored skeptic. Therefore, often when I remember the Gell-Mann Amnesia Effect, it is not in the context it was originally presented in but rather in relation to my everyday work of <em>inter-disciplinary</em> research. I am a computer scientist by training and I’ve spent the past half-decade working on applied machine learning research for the super niche scientific field of gravitational-wave astronomy as a PhD student. Exactly how I found myself in this position is a story for another time but the <em>AI for Science</em> movement is in full swing and this space is typically populated by domain scientists applying ML to the problems in their specific domains. So it follows that I, as a non-domain scientist, often find myself reading ML publications on topics in astrophysics. I read these research papers, often written by physicists who are not from the ML world, and I find myself spotting certain, umm, deficiencies in the information presented. Often it is the presentation style, rigor, ease of reproducibility, computation costs of the ML analysis, a sufficient justification for even using ML in the first place and so on. I tend to chart it up to the fact that the authors are not ML experts, take the information presented with a pinch of salt and move on. But in the same publication, when I read the physics-specific part of the paper, I find myself taking that information as gospel. Because…I am not a physicist and surely the physicist who wrote the paper must certinly be an expert in, well, physics! So, sometimes, I find myself thinking of the physicist who is likely treating the ML portions of the same publication as a gospel. You know the ones that made me raise an eyebrow?</p> <p>As a young student, I had assumed that the institutional systems of academia would be, by design, a bulwark against the Gell-Mann Amnesia Effect amongst academics. We are skeptics by nature and we are meant to talk to each other, provide feedback and rebuttals via a robust peer-review process and thus distill the signal from noise at the frontiers of research. But, alas, the silos of specialized disciplines populate the landscape and provide a fertile breeding ground for the Gell-Mann Amnesia to take root… even inside the ivory towers.</p>]]></content><author><name></name></author><category term="musings"/><summary type="html"><![CDATA[an occasional fear, a constant reminder.]]></summary></entry><entry><title type="html">how to do research in 10 “easy” steps</title><link href="https://rutujagurav.github.io/blog/2024/ten-steps/" rel="alternate" type="text/html" title="how to do research in 10 “easy” steps"/><published>2024-05-18T19:00:00+00:00</published><updated>2024-05-18T19:00:00+00:00</updated><id>https://rutujagurav.github.io/blog/2024/ten-steps</id><content type="html" xml:base="https://rutujagurav.github.io/blog/2024/ten-steps/"><![CDATA[<p>Cutting to the chase…</p> <ol> <li>💡Define/find a problem definition. <ul> <li>Ideally your advisors / collaborators will provide this along with reference paper(s) to get started with.</li> </ul> </li> <li>👓 Do a literature review and establish baselines in terms of existing methods and benchmark datasets. <ul> <li>Do 1-page/slide summaries of papers you read (Ref: <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">“How to Read a Paper”</a>). If there is a latest survey paper available, great! Start with that and save yourself some time.</li> <li>If it is an existing problem definition, there will be pre-existing datasets in the literature. If it is related-but-not-exactly-the-same problem definition, there will be related datasets (duh!). If it is custom/proprietary data, your advisors/collaborators should have provided you with it in step 1.</li> </ul> </li> <li>💭 Brainstorm contributions that can be made by you towards “solving” the problem definition to improve upon the baseline methods and/or benchmark datasets and/or evaluation/performance metrics to measure. <ul> <li>Methods contribution could be a novel method or an improvement of an existing method for scale/speed/robustness/explainability/etc.</li> <li>Datasets contribution could be a new dataset that would be valuable to current or future methods contributions for the problem definition or improving existing benchmarks in some meaningful way.</li> <li>Evaluation/performance metrics contribution could be new (or modification of existing) measures of how well the methods are doing towards solving the problem.</li> </ul> </li> <li>💁 Present the literature review and contributions <em>in context</em> of the problem definition. <ul> <li>Ideally listeners will tell you about the papers you might have missed and poke holes in your understanding of some literature (if they have read the papers too or are familiar with the topic) as well as your proposed contributions.</li> </ul> </li> <li>😩 Establish baseline methods performance on benchmark datasets. <ul> <li>Yes, you have to find implementations of baseline methods or implement them from scratch yourself and actually run the experiments to reproduce the reported performance numbers of baselines. DO NOT simply copy results tables from literature. Steps 1-5 can result in a survey paper.</li> </ul> </li> <li>😀 Realize contribution(s). <ul> <li>Define a new method and/or apply a previously unapplied set of existing methods to realize the proposed methods contributions.</li> <li>Apply a suite of established methods to the new datasets to realize the proposed datasets contributions.</li> <li>Report performance on the new evaluation/performance metrics to realize the proposed evaluation/performance metrics contributions.</li> </ul> </li> <li>💪 Verify correctness and compare against baselines. <ul> <li>Best to start with synthetic datasets to crash-test ideas and verify correctness of implementation(s).</li> <li>Usually a massive grid search is warranted at this stage to compare-and-contrast.</li> </ul> </li> <li>👓 Review Results. <ul> <li>Hopefully, the performance of the proposed solution is better and the proposed contributions are realized 😌. If not, go back to step 6 and improve the method 😧 or go back to step 3 and modify the contributions 😮 or, last resort, go back to step 1 to modify the problem definition and start over 💀.</li> <li>Silver Lining: If your literature review wasn’t too narrow it will still be useful if you have to start over with a modified problem definition. Worst case, you’ll at least knock out an interesting survey paper if you did a good lit. review and if the problem definition is a currently hot topic.</li> </ul> </li> <li>💁 Present results and get feedback from colleagues and collaborators. <ul> <li>Ideally listeners will poke holes in your experimental setup, sanity check your claims. They may suggest more experiments to make the claims stronger.</li> </ul> </li> <li>🐌 Write up a paper. Then go smoke a joint and rinse and repeat. <ul> <li>You should already have an Overleaf repo in Step 1 in a generic conference format with the typical sections of intro, background and motivation, proposed contributions and methodology, results, conclusion and future directions. This document will initially serve as a place to store your daily/weekly notes/drafts and over time it evolves into the final paper. Trust me, it can be quite overwhelming to start from a blank page at the end.</li> </ul> </li> </ol> <p><em>Disclaimer: The above is in the context of methods/applied research. I don’t know anything about theory research. I guess those folks begin by sacrificing a goat on a full moon, then do peyote and wait for some novel theory about some deep problem to pop into their mind… or more realistically go borrow some ideas from math and physics from 50 years ago. But then again, if you could read those math/physics papers and effectively translate their ideas into our domain, you wouldn’t be here reading this right now, would you?👀</em></p>]]></content><author><name></name></author><category term="my-little-helpers"/><category term="musings"/><summary type="html"><![CDATA[no, really.]]></summary></entry><entry><title type="html">what’s going on with sktime?</title><link href="https://rutujagurav.github.io/blog/2024/aeon-sktime-drama/" rel="alternate" type="text/html" title="what’s going on with sktime?"/><published>2024-04-18T23:00:00+00:00</published><updated>2024-04-18T23:00:00+00:00</updated><id>https://rutujagurav.github.io/blog/2024/aeon-sktime-drama</id><content type="html" xml:base="https://rutujagurav.github.io/blog/2024/aeon-sktime-drama/"><![CDATA[<p>It is super late at night and I have fallen into an internet rabbit hole of the timeseries machine learning community. I’ve been working with a lot of time domain data for a while now and of the several good python libraries available, <a href="https://www.sktime.net/en/stable/"><code class="language-plaintext highlighter-rouge">sktime</code></a> has been one that I haven’t quite used too much. My go-to ones have been - 1. <a href="https://tsfresh.readthedocs.io/en/latest/"><code class="language-plaintext highlighter-rouge">tsfresh</code></a> for the feature extraction capabilities, 2. <a href="https://tslearn.readthedocs.io/en/stable/"><code class="language-plaintext highlighter-rouge">tslearn</code></a> for standard machine learning algorithms, 3. <a href="https://stumpy.readthedocs.io/en/latest/"><code class="language-plaintext highlighter-rouge">stumpy</code></a> for all things matrix profile and 4. <a href="https://timeseriesai.github.io/tsai/"><code class="language-plaintext highlighter-rouge">tsai</code></a> for deep learning based algorithms.</p> <p>I have been playing around with a short project idea of examining clustering perfromance on all available timeseries datasets in the mighty UCR/UEA timeseries classification archive using various timeseries feature-sets like the ones available in <code class="language-plaintext highlighter-rouge">tsfresh</code> and the increasingly popular Catch-22 features available in <code class="language-plaintext highlighter-rouge">pycatch22</code> package<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, so imagine my delight when I went to <a href="https://timeseriesclassification.com">timeseriesclassification.com</a> to fetch all the datasets and read <em>“The scikit-learn compatible aeon toolkit contains the state of the art algorithms for time series classification. All of the datasets and results stored here are directly accessible in code using aeon.”</em> I thought, “Awesome, eveything I need in one place!”. So I go check out <a href="https://www.aeon-toolkit.org/en/stable/"><code class="language-plaintext highlighter-rouge">aeon</code></a> and it is fantastic. But my brain must have done a random access of some forgotten recess of my mind because I found myself thinking, “Huh, this looks familiar. Almost like <code class="language-plaintext highlighter-rouge">sktime</code>…”. And indeed that’s when I fell into the current rabbit hole from whence I write this post.</p> <p>Turns out there is some drama-llama stuff around this whole <code class="language-plaintext highlighter-rouge">sktime</code> vs. <code class="language-plaintext highlighter-rouge">aeon</code> saga. Turns out <a href="https://news.ycombinator.com/item?id=36432369"><code class="language-plaintext highlighter-rouge">aeon</code> is a fork of sktime</a> created by Tony Bagnall of UEA who departed(?) from <code class="language-plaintext highlighter-rouge">sktime</code> after <a href="https://github.com/sktime/community-org/issues/45">another core developer</a> allegedly took over the project and kicked others out(?) after fallout over some financial issues(?). The whole thing sounds like a bit of a mess. Anyhoo, I have no horse in the race. Preliminary examination suggests either library is fine for my purposes. I am going to go with <code class="language-plaintext highlighter-rouge">aeon</code> for the aforementioned short project. And this Alice needs to climb out of this hole and go to bed now.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I know, I know, some of the datasets available in the UCR/UEA archive are not amenable to features based classification perhaps and are more separable in terms of shapes or a combination of both but I digress. My plan is to see consensus between clustering and classification performance on the datasets. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="oh-what-a-mess"/><category term="uff"/><category term="musings"/><category term="sktime"/><category term="aeon"/><summary type="html"><![CDATA[the saga of sktime and aeon]]></summary></entry><entry><title type="html">convenience wrappers</title><link href="https://rutujagurav.github.io/blog/2024/wrappers-for-clf-and-clust/" rel="alternate" type="text/html" title="convenience wrappers"/><published>2024-04-15T15:00:00+00:00</published><updated>2024-04-15T15:00:00+00:00</updated><id>https://rutujagurav.github.io/blog/2024/wrappers-for-clf-and-clust</id><content type="html" xml:base="https://rutujagurav.github.io/blog/2024/wrappers-for-clf-and-clust/"><![CDATA[<h1 id="getting-started">Getting Started</h1> <p>Every time I start a new machine learning project (not deep learning, that’s a story for another time), I find myself going through the same tedious process of trial and error - set up a grid search to find the <em>right</em> model along with the <em>right</em> set of hyperparameters for the model that optimize one or more of the laundry list of <em>metrics-of-interest</em>…then repeat every combination of <em>free parameters</em> in this pipeline a bunch of times and finally making a lot of plots to get the lay of the land so to speak. So, over the years, I’ve developed a set of convenience wrappers around the mighty <code class="language-plaintext highlighter-rouge">scikit-learn</code> library to make this process a bit more streamlined and I’ve published them as <code class="language-plaintext highlighter-rouge">clfutils4r</code> for classification tasks and <code class="language-plaintext highlighter-rouge">clustutils4r</code> for clustering tasks (the ‘r’ being my initial and not the programming language…err, should have thought this through, huh?). I thought I would share them here in case they are useful to anyone else.</p> <h2 id="classification">Classification</h2> <p>The premise is this: Someone hands you a classification dataset. After you are done poking and prodding it with some exploratory data analysis (EDA), you want to know the standard metrics on various classifiers available in <code class="language-plaintext highlighter-rouge">scikit-learn</code> and you want to know them <em>now</em>. You don’t want to spend time writing boilerplate code setting up a grid search and you don’t want to spend time making plots to consolidate the results of said grid search. Here is minimally complete example of how you can do just that with 2 wrapper functions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1">## Load dataset: Example - breast cancer prediction
</span><span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_breast_cancer</span><span class="p">()</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">target_names</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">]</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">target</span>

<span class="c1">## Standardize features
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">## Split into train and test sets
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">## Grid search for best model
</span><span class="kn">from</span> <span class="n">clfutils4r.gridsearch_classification</span> <span class="kn">import</span> <span class="n">gridsearch_classification</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">gridsearch_results</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">best_model</span><span class="p">,</span> <span class="n">grid_search_results</span> <span class="o">=</span> <span class="nf">gridsearch_classification</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>                    <span class="c1"># training dataset
</span>                                                            <span class="n">gt_labels</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>            <span class="c1"># ground truth labels
</span>                                                            <span class="n">best_model_metric</span><span class="o">=</span><span class="sh">"</span><span class="s">F1</span><span class="sh">"</span><span class="p">,</span>       <span class="c1"># metric to use to choose the best model
</span>                                                            <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                    <span class="c1"># whether to display the plots; this is used in a notebook
</span>                                                            <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="n">save_dir</span>  <span class="c1"># whether to save the plots
</span>                                                        <span class="p">)</span>

<span class="c1">## Predict on test set
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">## Evaluate best model on test set
</span><span class="kn">from</span> <span class="n">clfutils4r.eval_classification</span> <span class="kn">import</span> <span class="n">eval_classification</span>
<span class="nf">eval_classification</span><span class="p">(</span><span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="o">=</span><span class="n">y_pred_proba</span><span class="p">,</span>  <span class="c1"># ground truth labels, predicted labels, predicted probabilities
</span>                    <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
                    <span class="n">make_metrics_plots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># make a variety of classification metrics plots
</span>                    <span class="n">make_shap_plot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shap_nsamples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="c1"># do Shapley analysis for model explainability
</span>                    <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  
                    <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">RESULTS_DIR</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getcwd</span><span class="p">()</span><span class="o">+</span><span class="sh">'</span><span class="s">/test_results</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Let’s dive a bit deeper into these two convenience functions.</p> <h3 id="gridsearch_classification"><code class="language-plaintext highlighter-rouge">gridsearch_classification</code></h3> <p>This will produce a whole bunch of useful outputs including the best model and the results of the grid search. The data is stored in neat folder structure in JSON files and is visualized with a <em>Parallel Co-ordinates Plot</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The parallel co-ordinates plot that is produced by gridsearch_classification() for the k-Nearest Neighbors classifier for the example above. </div> <h3 id="eval_classification"><code class="language-plaintext highlighter-rouge">eval_classification</code></h3> <p>The primary output is the classic sklearn <em>classification report</em>. Sometimes that’s all you need but by setting the <code class="language-plaintext highlighter-rouge">make_metrics_plots</code> to <code class="language-plaintext highlighter-rouge">True</code> you can choose to make a variety of other plots that I find useful for understanding the performance of the model. These include the familiar plots of the confusion matrix, the ROC curve, the precision-recall curve as well as some more exotic ones I found in <code class="language-plaintext highlighter-rouge">scikit-plot</code> that are exclusive to binary classification like the KS statistic plot, the cumulative gains curve and the lift curve. You can also choose to do Shapley analysis to <em>explain</em> the model predictions by setting the <code class="language-plaintext highlighter-rouge">make_shap_plot</code> parameter to <code class="language-plaintext highlighter-rouge">True</code> and specifying the number of samples to use for the analysis with the <code class="language-plaintext highlighter-rouge">shap_nsamples</code> parameter. I love the fantastic <code class="language-plaintext highlighter-rouge">shap</code> package so I just wrapped the <em>KernelExplainer</em> and <em>summary_plot</em> in this function.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/confusion_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_roc_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/classwise_pr_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The classic evaluation plots produced by eval_classification() on the test set for the best model returned by gridsearch_classification() for the example above. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/ks_stat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/cumul_gain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/lift_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Some more exotic evaluation plots exclusive to binary classification produced by eval_classification() on the test set for the best model returned by gridsearch_classification() for the example above. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/shap_summary_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Shapley analysis summary plot produced by shap package for the best model returned by gridsearch_classification() for the example above. </div> <h2 id="clustering">Clustering</h2> <p>Same premise as before but this time with clustering tasks. Here is a minimally complete example of how you can do it with 1 function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="n">clustutils4r.eval_clustering</span> <span class="kn">import</span> <span class="n">eval_clustering</span>

<span class="c1">## For testing purposes
</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span>

<span class="c1">### Synthetic data
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">save_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">results</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">best_model</span><span class="p">,</span> <span class="n">grid_search_results</span> <span class="o">=</span> <span class="nf">eval_clustering</span><span class="p">(</span>
                                       <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>                                               <span class="c1"># dataset to cluster
</span>                                       <span class="n">gt_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>                                       <span class="c1"># ground-truth labels; often these aren't available so don't pass this argument
</span>                                       <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>                                       <span class="c1"># number of times to fit a model
</span>                                       <span class="n">best_model_metric</span><span class="o">=</span><span class="sh">"</span><span class="s">Calinski-Harabasz</span><span class="sh">"</span><span class="p">,</span>             <span class="c1"># metric to use to choose the best model
</span>                                       <span class="n">make_silhoutte_plots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">embed_data_in_2d</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># whether to make silhouette plots
</span>                                       <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>                                         <span class="c1"># whether to display the plots; this is used in a notebook
</span>                                       <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">results</span><span class="sh">"</span>                      <span class="c1"># whether to save the plots
</span>                                    <span class="p">)</span>
</code></pre></div></div> <p>Clustering, as you know, is a bit trickier than classification because often there is no ground truth to compare the found clusters to. There is also rarely some external validation test or downstream task available to quantify if any/all clusters you found are “useful”. Clustering is often an exploratory tool. So, the evaluation is a bit more heuristic based. Clustering also often has one or more <em>free parameters</em>, for example, the number of clusters in case of partition-based algorithms like K-Means or the minimum cluster size in case of density based algorithms. In <code class="language-plaintext highlighter-rouge">sklearn</code> there are 3 intrinsic cluster quality metrics viz. the <em>Calinski-Harabasz</em> score, the <em>Davies-Bouldin</em> score and the most used one being the <em>Silhouette Score</em>. Another important evaluation folks do for clustering is measuring the consensus between different labellings of the same dataset. <code class="language-plaintext highlighter-rouge">sklearn</code> has a wide variety of clustering consensus metrics like <em>Adjusted Rand Index</em>, <em>Normalized Mutual Information</em>, <em>V Measure</em>, <em>Fowlkes-Mallows Index</em>.</p> <p>Let’s take a look at my convenience function.</p> <h3 id="eval_clustering"><code class="language-plaintext highlighter-rouge">eval_clustering</code></h3> <p>In the default setting in which all you have is the unlabelled dataset, it will calculate the three intrinsic cluster quality metrics for a variety of models and combinations of free parameters and return the best model based on the scoring metric you choose using the <code class="language-plaintext highlighter-rouge">best_model_metric</code> parameter along with the full grid search results. You can also make a <em>Silhouette Plot</em> for the best model by setting the <code class="language-plaintext highlighter-rouge">make_silhoutte_plots</code> to <code class="language-plaintext highlighter-rouge">True</code> and since most datasets have more than 2 features, you can get a t-SNE projection of the high dimensional datapoints by setting <code class="language-plaintext highlighter-rouge">embed_data_in_2d</code> to <code class="language-plaintext highlighter-rouge">True</code>. If you have the ground truth labels (or just another set of labels obtained by, say, a different clustering run) available, you can pass them to the <code class="language-plaintext highlighter-rouge">gt_labels</code> parameter and it will calculate the clustering consensus metrics.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/parcoord_plot_clust.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The parallel co-ordinates plot that is produced by eval_clustering() for the k-Means clustering from the example above. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot-480.webp 480w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot-800.webp 800w,/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_2024-04-15-wrappers-for-clf-and-clust/silhouette_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Silhouette plot that is produced by eval_clustering() for the best model from the example above. </div> <h2 id="parting-thoughts">Parting Thoughts</h2> <p>Every machine learning engineer, researcher and dabbler I’ve met over the years has their own version of such wrappers. I hope someone finds these ones useful. I’ve packaged and published these on PyPI; install them with <code class="language-plaintext highlighter-rouge">pip install clfutils4r</code> and <code class="language-plaintext highlighter-rouge">pip install clustutils4r</code>. The code is available on my Github, fork away and modify to your liking. Even if you don’t like them as they are, hopefully they save you some time by serving as a starting point. Recently, I’ve taken to using <a href="https://optuna.org/">Optuna</a> for hyperparameter optimization and I’m thinking of incorporating that into these wrappers. It has a lot cleverer ways of optimally searching the hyperparameters space than the good old GridSearchCV and RandomizedSearchCV that I’ve been using here. I would like to point to <a href="https://pycaret.org/">PyCaret</a> which is a fantastic low-code, scikit-learn wrapper library that does a lot of what I’ve done here and so much more; it has a truly eye-watering amount of options one can play with and it integrates pretty much every hyperparameter search package available including Optuna, Ray Tune, Hyperopt, Scikit Optimize (which is probably defunct now?). PyCaret has everything one would need for classification, especially the <code class="language-plaintext highlighter-rouge">compare_models()</code> function which is fantastic for getting a quick <em>models x metrics</em> table comparing all available models via cross-validation and <code class="language-plaintext highlighter-rouge">tune_model()</code> which essentially does what my <code class="language-plaintext highlighter-rouge">gridsearch_classification()</code> does and returns the best model and optionally the tuner object from which you can grab the full grid of results. As of this post, I haven’t seen similar functions in the clustering module of PyCaret but I’m sure they are on the way. I’ve used it a few times and it’s great but I wanted to write my own wrappers to understand the process better. Thanks for reading this post!</p>]]></content><author><name></name></author><category term="my-little-helpers"/><category term="classification"/><category term="scikit-learn"/><category term="wrappers"/><summary type="html"><![CDATA[wrappers for sklearn et al for classification and clustering]]></summary></entry></feed>